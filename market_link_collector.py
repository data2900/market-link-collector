import os
import sqlite3
from datetime import datetime
import scrapy

# â”€â”€ DB è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_PATH = os.getenv("MARKET_DB_PATH", os.path.abspath("./market_data.db"))

class MarketLinkSpider(scrapy.Spider):
    """
    ã‚µãƒ³ãƒ—ãƒ«ã®å¸‚å ´ç³»ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€
    ä¼æ¥­ã‚³ãƒ¼ãƒ‰ãƒ»ä¼æ¥­åãƒ»é–¢é€£ãƒªãƒ³ã‚¯ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰ã‚’ SQLite ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    name = "market_links"
    allowed_domains = ["example.com"]  # å›ºæœ‰ã®åª’ä½“åã¯ä¼ã›ã‚‹
    start_urls = ["https://example.com/markets/companies/"]  # ãƒ€ãƒŸãƒ¼URL

    def __init__(self, target_date=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # -a target_date=YYYYMMDD ã§å—ã‘å–ã‚‹ï¼ˆãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªç”¨ï¼‰
        if not target_date:
            raise ValueError("å®Ÿè¡Œæ™‚ã« -a target_date=YYYYMMDD ã®å½¢å¼ã§æ—¥ä»˜ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        try:
            datetime.strptime(target_date, "%Y%m%d")
        except ValueError:
            raise ValueError("æ—¥ä»˜ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„")
        self.target_date = target_date

        # SQLite åˆæœŸåŒ–
        self.conn = sqlite3.connect(DB_PATH)
        self.cursor = self.conn.cursor()
        self._init_db()

        # åé›†æ•°ã‚«ã‚¦ãƒ³ã‚¿
        self.total_seen = 0
        self.total_inserted = 0

    # â”€â”€ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _init_db(self):
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS consensus_url (
                target_date TEXT,
                code        TEXT,
                name        TEXT,
                link_a      TEXT,
                link_b      TEXT,
                link_c      TEXT,
                PRIMARY KEY (target_date, code)
            )
        """)
        self.conn.commit()

    # â”€â”€ çµ‚äº†æ™‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def closed(self, reason):
        msg = f"ğŸ“¦ å–å¾—ä»¶æ•°: {self.total_seen} / æ–°è¦ç™»éŒ²: {self.total_inserted}ï¼ˆdate={self.target_date}ï¼‰"
        self.logger.info(msg)
        print(msg)
        self.conn.close()

    # â”€â”€ è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ€ãƒŸãƒ¼ã®ä¸€èˆ¬çš„XPathï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def parse(self, response):
        # ä¸€èˆ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ§‹é€ ã‚’æƒ³å®š
        rows = response.xpath('//table//tr')
        for row in rows:
            # ä¾‹ï¼š1åˆ—ç›®ã«ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã€2åˆ—ç›®ã«ä¼æ¥­åãŒã‚ã‚‹ã¨ä»®å®š
            code = row.xpath('./td[1]//text()').get()
            name = row.xpath('./td[2]//text()').get()

            # ã‚³ãƒ¼ãƒ‰ã¨åç§°ãŒå–å¾—ã§ããŸè¡Œã®ã¿å‡¦ç†
            if code and name:
                self.total_seen += 1

                # ãƒ€ãƒŸãƒ¼ã®é–¢é€£ãƒªãƒ³ã‚¯ï¼ˆå®Ÿåœ¨ã‚µãƒ¼ãƒ“ã‚¹åã¯è¨˜è¼‰ã—ãªã„ï¼‰
                link_a = f"https://example.com/company/{code}"
                link_b = f"https://example.org/stock/{code}"
                link_c = f"https://example.net/detail/{code}"

                # INSERT OR IGNORE ã§é‡è¤‡å›é¿
                self.cursor.execute("""
                    INSERT OR IGNORE INTO consensus_url (
                        target_date, code, name, link_a, link_b, link_c
                    ) VALUES (?, ?, ?, ?, ?, ?)
                """, (self.target_date, code, name, link_a, link_b, link_c))

                if self.cursor.rowcount == 1:
                    self.total_inserted += 1

        self.conn.commit()

        # ã€ŒNextã€ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°å†å¸°è¿½è·¡ï¼ˆä¸€èˆ¬çš„ãªUIæ–‡è¨€ã‚’æƒ³å®šï¼‰
        next_page = response.xpath('//a[normalize-space(text())="Next"]/@href').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)
